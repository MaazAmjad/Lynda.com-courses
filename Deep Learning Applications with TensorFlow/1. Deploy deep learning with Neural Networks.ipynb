{
  "cells": [
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior() \n\n# Turn off TensorFlow warning messages in program output\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'",
      "execution_count": 95,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We set \"OS.Environ\" environment variable to two. This just tells TensorFlow not to output as many log messages as it normally does. By default, TensorFlow outputs a lot of log messages to your console when you run the program. The messages can sometimes be helpful, but we've turned them off for these demos to make the output easier to read. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "![Addition of two Tensors in Tensorflow](images/tn_add.png)"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The computational graph will look like. This graph has two inputs, X and Y. Those are the two numbers we want to add together. This graph also has one operation called 'addition'. That note simply adds together the tensors passed into it. Once this graph is defined, we can use it by creating a new TensorFlow session. Then we'll feed in values for X and Y, and we'll ask the session object to execute the addition node. The result we get back from executing the addition node will be the answer."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next, let's define the X and Y input nodes. When we create a node in TensorFlow, we have to choose what kind of node to create. The X and Y nodes will be placeholder nodes that get assigned a new value every time we make a calculation, so we'll create them as a TF.placeholder node and we'll create the first nodes by \"TF.placeholder\". When we create a placeholder node, we have to pass on the data type. We'll be adding numbers, so we can use a floating point data-type. We'll use TF.float32. We also need to give this node the name. The name will show up when we look at graphical visualizations of our model. We'll name this node X by passing in the parameter called 'name' with a value of X, and now let's define Y the same way. TF.placeholder, pass in the data-type, TF.float32 and give the name, Name Y. \n\nNow we can define the node that does the addition operation. In TensorFlow we can do that by creating a TF.Add node. Then we'll pass on the X and Y nodes to the addition node, that tells TensorFlow to link those nodes on the computational graph, so we're asking it to pull the values from X and Y and add the result. Let's also give the addition node the name=addition. That's the entire definition for our simple computational graph.\n\nThe model only has three nodes, so we can define it with just three lines of code but when you're building more complex models, this code can get quite complicated."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Define computational graph\nX = tf.placeholder(tf.float32, name=\"X\")\nY = tf.placeholder(tf.float32, name=\"Y\")\n\naddition = tf.add(X, Y, name=\"addition\")",
      "execution_count": 96,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To execute operations in the graph, we have to create a session with \"tf.session as session\". Now that we have a session, we can ask the session to run operations on their computational graph by calling \"Session.run\". Now, we'll say result=session.run, then we need to pass on the operation we want to run, in this case addition. When the addition operation runs, it's going to see that it needs to grab the values of the X and Y nodes, so we also need to feed in values for X and Y. We can do that by supplying a parameter called 'feed_dict' parameter and then we'll pass in X and give them the array values of [1, 2, 9] and Y, and pass in a value of [3,7,18] as an array. And then finally we'll just print the result."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create the session\nwith tf.Session() as session:\n\n    result = session.run(addition, feed_dict={X: [1, 2, 9], Y: [3, 7, 18]})\n\n    print(result)",
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[ 4.  9. 27.]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We can see we got the right answer, notice that we passed in the arrays for the values of X and Y, and the result we got back from TensorFlow is also in the array. That's because TensorFlow always works with tensors, which are multi-dimensional arays. It's expecting us to feed in the arrays and it will return arrays, that means we can feed in multiple numbers (multi-dimensional arrays) at once for X and Y because we aren't just adding together two numbers, we're actually adding together two tensors. \n\nThis might seem like a lot of work to add some numbers together, just to do addition, we had to define the model, create a session, pass in data, and then send it off to the TensorFlow execution engine and wait for the result. But TensorFlow's real value is when we are working with large data sets and computationally intensive operations. It can take the same operational graph we've defined here, and execute it across multiple machines who are using graphics cards with GPUs to accelerate processing. The same code in TensorFlow can scale from running on a low power device like a cell phone, all the way up to running on multiple servers in a massive data center."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Problem statement: Write an algorithm that will predict how much money we can expect in selling future video games?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Import Libraries"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior() \n\ntf.reset_default_graph() \n#In the case you want to re-run codeblock 3 (for what ever reason) just insert a simple tf.reset_default_graph()\n#at the beginning of the block. \n#This will reset the graph you have already create and though you can create it again.\n",
      "execution_count": 114,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Data Analysis\n### Loading data"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The data set of video games sold by an imaginary video game retailer. We'll use this data to train the neural network that will predict how much money we can expect future video games to earn based on our historical data. First, let's open up the data and take a look at it. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Load training data set from CSV file\ntraining_data_df = pd.read_csv(\"sales_data_training.csv\", dtype=float)\ntraining_data_df.head()",
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 115,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>critic_rating</th>\n      <th>is_action</th>\n      <th>is_exclusive_to_us</th>\n      <th>is_portable</th>\n      <th>is_role_playing</th>\n      <th>is_sequel</th>\n      <th>is_sports</th>\n      <th>suitable_for_kids</th>\n      <th>total_earnings</th>\n      <th>unit_price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3.5</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>132717.0</td>\n      <td>59.99</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.5</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>83407.0</td>\n      <td>49.99</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>62423.0</td>\n      <td>49.99</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.5</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>69889.0</td>\n      <td>39.99</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>161382.0</td>\n      <td>59.99</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "   critic_rating  is_action  is_exclusive_to_us  is_portable  is_role_playing  \\\n0            3.5        1.0                 0.0          1.0              0.0   \n1            4.5        0.0                 0.0          0.0              0.0   \n2            3.0        0.0                 0.0          0.0              0.0   \n3            4.5        1.0                 0.0          0.0              0.0   \n4            4.0        1.0                 0.0          1.0              0.0   \n\n   is_sequel  is_sports  suitable_for_kids  total_earnings  unit_price  \n0        1.0        0.0                0.0        132717.0       59.99  \n1        1.0        1.0                0.0         83407.0       49.99  \n2        1.0        1.0                0.0         62423.0       49.99  \n3        0.0        0.0                1.0         69889.0       39.99  \n4        1.0        0.0                1.0        161382.0       59.99  "
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "For each video game, we've recorded several attributes. First, we have critic_rating, which is an average star rating out of the five stars. Next, is_action, which tells us if this is an action game, is_exclusive_to_us, which tells us if we have an exclusive deal to sell this game, is_portable, which tells us if this game runs on the handheld video game player, is_role_playing, which tells us this is a role-playing game, which is a genre of video games, is_sequel, which tells us this game was a sequel of an earlier video game, is_sports, which tells us this is a sports video game, suitable_for_kids, which tells us this game is appropriate for all ages. We also have total_earnings, which is how much total money we earned from selling this game to all customers, and unit_price, which tells us how much money one copy of the game retailed for. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "dataset.shape",
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 116,
          "data": {
            "text/plain": "(1000, 10)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "It means that we have 1000 rows (1000 games we sold, 10 means 10 attributes that atleast one game has some attributes out of 100)"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now we need to repeat the exact same process to load the test data set. Again, we'll use pandas read_csv function to load the testing data."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Load testing data set from CSV file\ntest_data_df = pd.read_csv(\"sales_data_test.csv\", dtype=float)\ntest_data_df.head()",
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 117,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>critic_rating</th>\n      <th>is_action</th>\n      <th>is_exclusive_to_us</th>\n      <th>is_portable</th>\n      <th>is_role_playing</th>\n      <th>is_sequel</th>\n      <th>is_sports</th>\n      <th>suitable_for_kids</th>\n      <th>total_earnings</th>\n      <th>unit_price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3.5</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>247537.0</td>\n      <td>59.99</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.5</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>73960.0</td>\n      <td>59.99</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.5</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>82671.0</td>\n      <td>59.99</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>137456.0</td>\n      <td>39.99</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>89639.0</td>\n      <td>59.99</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "   critic_rating  is_action  is_exclusive_to_us  is_portable  is_role_playing  \\\n0            3.5        1.0                 1.0          1.0              0.0   \n1            2.5        0.0                 0.0          0.0              1.0   \n2            3.5        0.0                 0.0          0.0              0.0   \n3            4.0        1.0                 1.0          0.0              0.0   \n4            2.0        1.0                 0.0          1.0              0.0   \n\n   is_sequel  is_sports  suitable_for_kids  total_earnings  unit_price  \n0        1.0        0.0                1.0        247537.0       59.99  \n1        1.0        0.0                0.0         73960.0       59.99  \n2        1.0        1.0                0.0         82671.0       59.99  \n3        1.0        0.0                0.0        137456.0       39.99  \n4        1.0        0.0                0.0         89639.0       59.99  "
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next we need to split the training data into two groups, the X group is data about each video game that we'll pass into the neural network, and the Y group are the values we want to predict. In this case, that's how much money we think each video game will earn. \n\nTo create the X training data, we'll take all the columns in the training data, and simply drop the total_earnings column. That will give us all the columns except the one we don't want. So we'll call training_data_df.drop, and we'll pass in the name of the column to drop, total_earnings. Then we also need to pass in an axis=1 parameter that tells it we want to drop a column and not a row. Finally, we'll call .values to get back the result as an array.\n\nTo create the Y array, we will only grab the total_earnings column, so we'll call training_data_df, and then we'll pass in the total_earnings column, and then we'll call .values to get the result as an array. \n\nwe need to repeat the exact same process to load the test data set."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Pull out columns for X (data to train with) and Y (value to predict)\nX_training = training_data_df.drop('total_earnings', axis=1).values\nY_training = training_data_df[['total_earnings']].values\n\n\n# Pull out columns for X (data to train with) and Y (value to predict)\nX_testing = test_data_df.drop('total_earnings', axis=1).values\nY_testing = test_data_df[['total_earnings']].values",
      "execution_count": 118,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"Total Training Samples:\", X_training.shape)\nprint(\"Total Training labels:\", Y_training.shape)\nprint(\"Total Testing Samples:\", X_testing.shape)\nprint(\"Total Testing labels:\", Y_testing.shape)",
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Total Training Samples: (1000, 9)\nTotal Training labels: (1000, 1)\nTotal Testing Samples: (400, 9)\nTotal Testing labels: (400, 1)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Data Pre-processing\n\nNow we need to pre-process our data. In order to train the neural network, we want to scale all the numbers in each column of our data set to be between the value of 0 and 1. If the numbers in one column are large but the numbers in another column are small, the neural network training won't work very well. One way we can do this is to use the MinMaxScaler object from the popular scikit-learn library. It's designed for exactly this purpose. So on line 21, first we'll create a new MinMaxScaler. We just need to pass in a feature_range parameter, which tells it that we want all numbers scaled between 0 and 1. So we'll say feature_range= and then pass in a tuple (0, 1), and then we'll do the same thing for the Y_scaler MinMaxScaler(feature_range=(0, 1))."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# All data needs to be scaled to a small range like 0 to 1 for the neural\n# network to work well. Create scalers for the inputs and outputs.\nX_scaler = MinMaxScaler(feature_range=(0, 1))\nY_scaler = MinMaxScaler(feature_range=(0, 1))",
      "execution_count": 120,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To actually scale our data, we call the fit_transform function on the scaler object, and pass in the data we want to scale. The result will be the rescaled data. we'll pass in X_training as the data to scale. And then we'll do the same for the Y_scaled_training data. We'll say Y_scaler.fit_transform, and we'll pass in the Y_training data. Fit_transform means we want it to first fit to our data, or figure out how much to scale down the numbers in each column, and then we want it to actually transform, or scale the data. Now we need to scale the test data in the same way. We want to make sure the test data is scaled by the same amount as the training data. So in this case, we'll just call transform on each scaler object, instead of fit_transform. So, we'll call X_scaler.transform, and we'll pass in the X_testing data, and then on line 30, Y_scaler.transform, and we'll pass in the Y_testing data. And now our data is ready to pass in the TensorFlow.The scaler scales the data by multiplying it by a constant number and adding a constant number. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Scale both the training inputs and outputs\nX_scaled_training = X_scaler.fit_transform(X_training)\nY_scaled_training = Y_scaler.fit_transform(Y_training)\n\n# It's very important that the training and test data are scaled with the same scaler.\nX_scaled_testing = X_scaler.transform(X_testing)\nY_scaled_testing = Y_scaler.transform(Y_testing)\n\nprint(X_scaled_testing.shape)\nprint(Y_scaled_testing.shape)",
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": "(400, 9)\n(400, 1)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now, we printed out how the Y data (it means the total earning) was scaled. We can get these values by looking at the Y_scaler.scale_[0] Object, and the Y_scaler.min[0] Object. \n\n    - Y_scaler.scale_[0] tells the constant that was multiplied to each value to scale the data. \n    - Y_scaler.min[0] tells the constant that was added to each value to scale the data.\n\n\n\nThis will be useful to know later, when we want to make predictions with the neural network and be able to unscale the data back to the original units. We can see that the data was scaled by these two values."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"Note: Y values were scaled by multiplying by {:.10f} and adding {:.4f}\".format(Y_scaler.scale_[0], Y_scaler.min_[0]))",
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Note: Y values were scaled by multiplying by 0.0000036968 and adding -0.1159\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**We'll use TensorFlow to build the neural network that tries to predict the total earnings of a new game, based on these characteristics.** "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Define the model\n\nLet's build a neural network with TensorFlow. Our training data set has nine input features, so we'll need nine inputs in our neural network. We can model that with a placeholder called X that holds nine values. Then, let's have three layers in their neural network that will train to find the relationship between the inputs and the output. There are many different types of layers you can use in the neural network, but we're going to use the most straightforward type, a fully connected neural network layer. That means that every node in each layer is connected to every node in the following layer. The first layer will have 50 nodes, the second layer will have 100 nodes, and the third layer will have 50 nodes again. To me, these layer sizes seem like a good starting point, but it's just a guess. Once the neural network is coded we can test out different layer sizes to see what layer size gives us the best accuracy. And since we are trying to predict a single value, we'll have an output layer with just one node, that will be out prediction. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "![Addition of two Tensors in Tensorflow](images/model.png)"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now we define variables with how many input and output nodes we'll have. That'll make it easy to adjust these numbers later if needed. So we'll have nine input nodes, so put nine. We'll have one output, so put one. Let's also create variables for how many nodes we want in each layer of our neural network. So for layer one nodes we'll have 50, and for layer two nodes we'll have 100. And then for layer three nodes we'll have 50 again. Now we are ready to start building the neural network itself."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Define how many inputs and outputs are in our neural network\nnumber_of_inputs = 9\nnumber_of_outputs = 1\n\n\n# Define how many neurons we want in each layer of our neural network\nlayer_1_nodes = 50\nlayer_2_nodes = 100\nlayer_3_nodes = 50",
      "execution_count": 123,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "First, let's define the input layer of our neural network starting. To keep things organized it's helpful to put each layer of our neural network in its own variable scope. Normally in Python we organize our code by creating new functions. In TensorFlow we can create variable scopes by using the tf.variablescope function instead. Any variables we create within this scope will automatically get a prefix of input to their name internally in TensorFlow. TensorFlow has the ability to generate diagrams of the computational graph. By putting our nodes into scopes it helps TensorFlow generate more useful diagrams that are easier to understand. Everything within the same scope will be grouped together within the diagram. Our neural network should accept nine floating point numbers as the input for making predictions. But each time we want a new prediction the specific values we pass in will be different. So we can use a placeholder node to represent that. So for X the input to our neural network, we will write as tf.placeholder object. When we create a new node we need to tell it what type of tensor it will accept. The data we are passing into our network will be floating point numbers, so we'll tell it to inspect the tf.float32 object. We also need to tell it the size or the shape of the tensor to expect. For the shape of the input, we use None, number_of_inputs. So we'll say shape = and then a tuple (None, number_of_inputs). None tells TensorFlow our neural network can mix up batches of any size and number_of_inputs tells it to expect 9 values for each record in the batch."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Section One: Define the layers of the neural network itself\n\n# Input Layer\nwith tf.variable_scope('input'):\n    X = tf.placeholder(tf.float32, shape=(None, number_of_inputs))",
      "execution_count": 124,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We already defined number_of_inputs above.\n\nNow, let's define the first layer of our neural network. We'll start with a new variable_scope. We'll call this one layer_1. Each fully connected layer of the neural network has three parts:\n    - A bias value for each node\n    - A weight value for each connection between each node and the nodes in the previous layer\n    - An activation function that outputs the result of the layer.\n    \nFirst, we need variables to store the bias values for each node. This will be a variable instead of a placeholder because we want TensorFlow to remember the value over time. To create a variable we can call tf.getvariable and pass in the name for the bias value like name=\"biases1\". Next, we need to pass in the shape of this variable. There's one bias value for each node in this layer, so the shape should be the same as the number of nodes in the layer. We define that above as Layer_1_nodes. So we'll say shape=, and then we'll pass in the size as Layer_1_nodes. We also need to tell TensorFlow the initial value of this variable. We can tell TensorFlow how to set the initial value of a variable by passing it one of TensorFlow's built-in initializer functions. We want the bias values for each node to default to zero, so I'll pass in the inititalizer function of tf.zeros_initializer as the initializer to use. So we'll pass in initializer=, and pass in tf.zeros_initializer. \n\n\nNext let's create a variable to hold the weights for this layer. Again we'll the tf.get_variable to create a variable. We'll pass in the name weight1. Name= \"weight1\", we also need to give this variable a shape. This part's a little tricky. We want to have one weight for each node's connection to each node in the previous layer. So we can define the shape like this. We'll say shape=, and as an array, one side of the array will be number of inputs, and the other side will be layer_1_nodes. And finally, we need to set the variable initializer. With neural networks, a lot of research has gone into the best initial values to use for weights. A good choice is an algorithm called **Xavier initialization is same as tf.glorot_uniform_initializer()** and TensorFlow has a built-in xavier_initializer function so we can use that. So we'll pass in initializer=, tf.contrib.layers.xavier_initializer.\n\nThe last part of defining this layer is multiplying the weights by the inputs and calling an activation function. TensorFlow is very flexible here and let's you do this in any way you want. We're going to use matrix multiplication and a standard rectified linear unit or relu activation function. We can do this by first calling tf.matmul for matrix multiplication. And we'll multiply the inputs, X, by the weights in this layer. To that we'll add the biases, and we'll wrap that with a call to the relu function. Tf.nn.relu. This is how the standard fully-connected neural network is defined. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Alright, we can define the second layer in a similar way. So first we will copy the definition of layer one, and then we will paste it under layer two. And now we need to change all the ones to twos. \n\nFro example,  Weights1 = weights2. Layer_1_nodes =  layer_2_nodes and so on. Biases2, Layer_2_nodes. Change the output to layer_2_output. We also need to change the shape of the weights so the input now is the output from the previous layer. So here we'll use layer_1_nodes instead of number of inputs. And then when we calculate the output, instead of multiplying X, which is the initial input, we want to multiply by the previous layer. So we'll multiply by layer_1_output. \n\n\nThen we can define the third layer in the same way, so we'll change all the twos to threes. Weights3, change layer_1_nodes to layer_2_nodes in this case. Layer_2_nodes to layer_3_nodes. Biases3, layer_3_nodes, the output will be layer_3_output, and in this case we'll be multiplying the weights for this layer by the output for layer two, so here we'll use layer_2_output. \n\nNow we're ready to define the output layer. It's similar but slightly different. So We'll call it weights4 and biases4, and we'll change layer_two_nodes to layer_three_nodes, but instead of having the shape be layer_four_nodes, we're going to use the number_of_outputs variable that we defined earlier since this is the final layer. And the same for the biases. Then again we want to multiply the weights from this layer by the output from layer three. And then for the final output, instead of calling it layer_three_output, let's just call it prediction. Since this is the final output from our network, that'll make it easier to follow. Right now we have the entire neural network defined, but we don't have any way to train it yet."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Layer 1\nwith tf.variable_scope('layer_1'):\n    weights = tf.get_variable(name=\"weights1\", shape=[number_of_inputs, layer_1_nodes], initializer=tf.glorot_uniform_initializer())\n    biases = tf.get_variable(name=\"biases1\", shape=[layer_1_nodes], initializer=tf.zeros_initializer())\n    layer_1_output = tf.nn.relu(tf.matmul(X, weights) + biases)\n\n# Layer 2\nwith tf.variable_scope('layer_2'):\n    weights = tf.get_variable(name=\"weights2\", shape=[layer_1_nodes, layer_2_nodes], initializer=tf.glorot_uniform_initializer())\n    biases = tf.get_variable(name=\"biases2\", shape=[layer_2_nodes], initializer=tf.zeros_initializer())\n    layer_2_output = tf.nn.relu(tf.matmul(layer_1_output, weights) + biases)\n\n# Layer 3\nwith tf.variable_scope('layer_3'):\n    weights = tf.get_variable(name=\"weights3\", shape=[layer_2_nodes, layer_3_nodes], initializer=tf.glorot_uniform_initializer())\n    biases = tf.get_variable(name=\"biases3\", shape=[layer_3_nodes], initializer=tf.zeros_initializer())\n    layer_3_output = tf.nn.relu(tf.matmul(layer_2_output, weights) + biases)\n\n# Output Layer\nwith tf.variable_scope('output'):\n    weights = tf.get_variable(name=\"weights4\", shape=[layer_3_nodes, number_of_outputs], initializer=tf.glorot_uniform_initializer())\n    biases = tf.get_variable(name=\"biases4\", shape=[number_of_outputs], initializer=tf.zeros_initializer())\n    prediction = tf.matmul(layer_3_output, weights) + biases",
      "execution_count": 125,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To be able to train it we need a cost function. \n\n**A cost function**, also called a lost function tells us how wrong the neural network is when trying to predict the correct output for a single piece of training data.  we have another scope called cost. Next, we'll define Y, a node for the expected value that we'll feed in during training. Just like the input values it will be a placeholder node because we'll feed in a new value each time. So we'll create a tf.placeholder node. The type will be floating point again, so we'll use tf.float32. And for the shape in this case we'll pass in None, 1 since there's just one single output. Shape equals None, 1. \n\nNext we need to calculate the cost. To measure the cost we'll calculate the mean squared error between what the neural network predicted and what we expected it to calculate. To do that we'll call the tf.squared_difference function and pass in the actual predication and the expected value. So tf.squared_difference, and we'll pass in our prediction which we defined above. And our expected value, which is Y. That will give us the square difference. To turn it into a mean square difference, we want to get the average value of that difference. So we'll wrap that with a call to the reduce_mean function which will do that for us. So we'll wrap that with a call to tf.reduce_mean. And that's it. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Section Two: Define the cost function of the neural network that will measure prediction accuracy during training\n\nwith tf.variable_scope('cost'):\n    Y = tf.placeholder(tf.float32, shape=(None, 1))\n    cost = tf.reduce_mean(tf.squared_difference(prediction, Y))",
      "execution_count": 126,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The very last step is to create an optimizer operation that TensorFlow can call to train the network. As usual, let's create a new scope, and we can call this one train. To define an optimizer, we just need to call one of the optimizers supplied by TensorFlow. One of the standard optimizers that's very powerful is called the **Adam optimizer**. To use it just call tf.train.AdamOptimizer.\n\nWe just need to pass in the learning rate. Next, we need to tell it which variable we want it to minimize. So we'll pass in our cost function as the value to minimize call. So we'll call.minimize, and we'll pass in cost. That tells TensorFlow that whenever we tell it to execute the optimizer, it should run one iteration of the Adam optimizer in an attempt to make the cost value smaller. And that's it, now our neural network is fully defined."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Section Three: Define the optimizer function that will be run to optimize the neural network\n\n# Define model parameters\nlearning_rate = 0.001\ntraining_epochs = 100\ndisplay_step = 5\n\n\nwith tf.variable_scope('train'):\n    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)",
      "execution_count": 127,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": " Once you have loaded your data set and defined your model, you're ready to create a training loop to train the model. Here, we've already defined the variable called training_epochs.\n \nAn epoch is another name for one full training pass over the training data set. This means we will do 100 iterations in our training loop to train our neural network. Okay, let's get started building the training loop itself.\n\nTo run any operation on the TensorFlow graph, you first need to create a session. We can create a new session by calling tf.Session. Within a session, we can ask TensorFlow to execute commands by calling session.run. Session.run, and then we can pass in the command we want TensorFlow to execute. Those can either be global commands that TensorFlow provides or specific nodes in our graph that we want to execute. The first command we always run is the built in command to tell TensorFlow to initialize all variables in our graph to their default values. The command that's called tf.global_variables_initializer. Global. Now that all the variables in our graph are initialized, we're ready to create our training loop. To train our neural network, we'll run its optimizer function over and over in the loop, either a fixed number of times or until it hits an accuracy level we want.\n\nAs we have defined \"training_epochs = 100\". This loop will execute 100 times. Inside the loop we'll tell TensorFlow to execute a single training pass over the training data by calling the optimizer function. We can do this by calling session.run and then pass in a reference to the operation that we want to call. In this case, that's the optimizer operation that we defined above. The optimizer needs some additional data to run. It needs the training data and the expected results for this training pass. In our computational graph, we have a placeholder node called x that accepts the training data and a placeholder node called y that accepts the expected results. To feed values into a placeholder node, we can pass them in as a parameter called feed_dict. So'll say feed_dict= ans pass parameters as a Python dictionary (because it only accepts dictionary) where we pass in the names of the placeholder nodes we want to feed data into, and the values we want to feed in. So let's pass in our scaled training data as x and our scaled expected output as y. So for X, and we'll pass x_scaled_training and for y, we'll pass y_scaled_training."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now, let's print out a message each time we do a training pass and print out the message when training is complete. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Initialize a session so that we can run TensorFlow operations\nwith tf.Session() as session:\n\n    # Run the global variable initializer to initialize all variables and layers of the neural network\n    session.run(tf.global_variables_initializer())\n\n    # Run the optimizer over and over to train the network.\n    # One epoch is one full run through the training data set.\n    for epoch in range(training_epochs):\n\n        # Feed in the training data and do one step of neural network training\n        session.run(optimizer, feed_dict={X: X_scaled_training, Y: Y_scaled_training})\n\n        # Print the current training status to the screen\n        print(\"Training pass: {}\".format(epoch))\n\n    # Training is now complete!\n    print(\"Training is complete!\")",
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Training pass: 0\nTraining pass: 1\nTraining pass: 2\nTraining pass: 3\nTraining pass: 4\nTraining pass: 5\nTraining pass: 6\nTraining pass: 7\nTraining pass: 8\nTraining pass: 9\nTraining pass: 10\nTraining pass: 11\nTraining pass: 12\nTraining pass: 13\nTraining pass: 14\nTraining pass: 15\nTraining pass: 16\nTraining pass: 17\nTraining pass: 18\nTraining pass: 19\nTraining pass: 20\nTraining pass: 21\nTraining pass: 22\nTraining pass: 23\nTraining pass: 24\nTraining pass: 25\nTraining pass: 26\nTraining pass: 27\nTraining pass: 28\nTraining pass: 29\nTraining pass: 30\nTraining pass: 31\nTraining pass: 32\nTraining pass: 33\nTraining pass: 34\nTraining pass: 35\nTraining pass: 36\nTraining pass: 37\nTraining pass: 38\nTraining pass: 39\nTraining pass: 40\nTraining pass: 41\nTraining pass: 42\nTraining pass: 43\nTraining pass: 44\nTraining pass: 45\nTraining pass: 46\nTraining pass: 47\nTraining pass: 48\nTraining pass: 49\nTraining pass: 50\nTraining pass: 51\nTraining pass: 52\nTraining pass: 53\nTraining pass: 54\nTraining pass: 55\nTraining pass: 56\nTraining pass: 57\nTraining pass: 58\nTraining pass: 59\nTraining pass: 60\nTraining pass: 61\nTraining pass: 62\nTraining pass: 63\nTraining pass: 64\nTraining pass: 65\nTraining pass: 66\nTraining pass: 67\nTraining pass: 68\nTraining pass: 69\nTraining pass: 70\nTraining pass: 71\nTraining pass: 72\nTraining pass: 73\nTraining pass: 74\nTraining pass: 75\nTraining pass: 76\nTraining pass: 77\nTraining pass: 78\nTraining pass: 79\nTraining pass: 80\nTraining pass: 81\nTraining pass: 82\nTraining pass: 83\nTraining pass: 84\nTraining pass: 85\nTraining pass: 86\nTraining pass: 87\nTraining pass: 88\nTraining pass: 89\nTraining pass: 90\nTraining pass: 91\nTraining pass: 92\nTraining pass: 93\nTraining pass: 94\nTraining pass: 95\nTraining pass: 96\nTraining pass: 97\nTraining pass: 98\nTraining pass: 99\nTraining is complete!\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We can see that it runs 100 loops of training. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The training seems to have completed successfully. But right now, we don't have a way to see if the accuracy is improving over time during training. So, we want to see how the training is progressing. Every five training steps, let's print out the current accuracy. \n\nIf epoch %5 == zero: this simply says that for every five passes in the training loop, we want to do something. To get the current accuracy, we can run the neural network's cost function and print out the result. So we'll call session.run and ask it to call the cost function. So we'll say training_cost = and we'll call session.run, then we'll pass in the cost function. To call the cost function, we need to pass in a feed dict containing the training data, so we'll say, feed_dict equals, and for X we'll pass in the X-scaled training data, and we'll do the same for Y, passing in the Y-scaled training data. And that will give us the current cost with the training data."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Initialize a session so that we can run TensorFlow operations\nwith tf.Session() as session:\n\n    # Run the global variable initializer to initialize all variables and layers of the neural network\n    session.run(tf.global_variables_initializer())\n\n    # Run the optimizer over and over to train the network.\n    # One epoch is one full run through the training data set.\n    for epoch in range(training_epochs):\n\n        # Feed in the training data and do one step of neural network training\n        session.run(optimizer, feed_dict={X: X_scaled_training, Y: Y_scaled_training})\n\n        # Every 5 training steps, log our progress\n        if epoch % 5 == 0:\n            training_cost = session.run(cost, feed_dict={X: X_scaled_training, Y:Y_scaled_training})\n            testing_cost = session.run(cost, feed_dict={X: X_scaled_testing, Y:Y_scaled_testing})\n\n            print(epoch, training_cost, testing_cost)\n\n    # Training is now complete!\n    print(\"Training is complete!\")\n    \n    #We can also monitor the current cost with the testing data the same way, just by passing in the testing data instead.\n    \n    final_training_cost = session.run(cost, feed_dict={X: X_scaled_training, Y: Y_scaled_training})\n    final_testing_cost = session.run(cost, feed_dict={X: X_scaled_testing, Y: Y_scaled_testing})   ",
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": "0 0.1565808 0.17097369\n5 0.014423772 0.015644506\n10 0.033309635 0.03356239\n15 0.011114591 0.0114575755\n20 0.009019991 0.0100311255\n25 0.009765232 0.010789294\n30 0.0057321517 0.006084182\n35 0.0050926046 0.0050552855\n40 0.0043443334 0.004247127\n45 0.0030190912 0.0030657693\n50 0.0028118526 0.0029401106\n55 0.0020936108 0.0020677499\n60 0.0018248911 0.0017626066\n65 0.0013986166 0.0014243898\n70 0.0011680999 0.0012278496\n75 0.0009659473 0.00096647727\n80 0.0008029182 0.00084231776\n85 0.00070455327 0.0007736494\n90 0.00061326264 0.0006536889\n95 0.0005340129 0.0005918348\nTraining is complete!\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"Final Training cost: {}\".format(final_training_cost))\nprint(\"Final Testing cost: {}\".format(final_testing_cost))",
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Final Training cost: 0.0004817887966055423\nFinal Testing cost: 0.0005423743859864771\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We can see how the cost decreases at each step in the training process, all the way down towards the end. That means the neural network is getting more accurate over time with training. And here's the final cost, 0.00048. That's not bad. We'll of course see a slightly different value, since there's randomness involved with initializing and training a neural network (if we rerun the code). But our values should be relatively close."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Tensorflow Feature Board"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In TensorFlow, it can be difficult to visualize exactly what's happening during the training process. Luckily, TensorFlow provides TensorBoard, a web-based interface that lets us visualize and monitor our machine-learning model. One of the most useful features of TensorBoard is that it lets us track the accuracy of our model as it trains. The first tab of TensorBoard is called Scalars. The term scalar here just means a single value as opposed to an array of multiple numbers. This is the section of TensorBoard where you can log single values over time and view the results as graphs.\n\n\nIn TensorBoard, the chart shows the value of the cost function over time while training the neural network. Being able to visualize our data is very helpful. However, we have to tell TensorFlow to log the values we want to visualize. It won't automatically create charts for us.\n\nLet's see how to add logging to a neural network. We're printing out the cost to the console as we train. But we're not saving any log files yet so TensorBoard won't be able to show us anything. In TensorFlow, we log values by creating special operations in our graph called summary operations. These operations take in the value and create log data in a format that TensorBoard can understand. Then, we pass that summary data to a TensorFlow file writer object to save it to disk. \n\nFirst, let's add the summary operation to our computational graph that will log the cost. To keep things organized, we've created a new variable scope to hold our logging operations. We've just called it logging. Now, let's add in a tf.summary.scalar object that will represent the value we are logging. So we'll say, tf.summary.scalar. To log the value, we pass in the name, let's call this current_cost, and then, a reference to the operation to log. In this case, cost. \n\nSo why did we use a tf.summary.scalar object? A scalar is just a single number so that's what we want to use to log a single value like this. TensorFlow also supports logging more complex objects like histograms, pictures, and even sound files but most of the time, we'll be logging single numbers. We can run this node by calling session.run on it just like any other node in our graph. Running it will generate the log data in the right format. But while running this node directly is easy when we only have one metric, sometimes we'll want to log many different metrics. It can be tedious to have to call session.run on every single metric so TensorFlow has a shortcut. We're going to define a new node of type tf.summary.merge_all. So we'll say summary equals tf.summary.merge_all. When you run this special node, it'll automatically execute all the summary nodes in your graph without having to explicitly list them all. It's just a helper that makes life easier. \n\nNow, let's move down to the training loop. We also need to create the log files to save this data to. We can do that by creating a tf.summary.FileWriter object. First, we'll create one to write out our training accuracy. So we'll say, training_writer equals tf.summary.FileWriter. You just need to pass in the name of the folder you want to save the file to. So, we will use /logs/training. And then, we need to pass in the reference to our computational graph, that's just session.graph. And then, we'll create a separate file the same way to save our testing accuracy. So that's tf.summary.FileWriter. In this case, we'll use the path of /logs/testing. And then, again, we'll pass in session.graph. Notice that we put both log file in the same logs subfolder. If we put multiple log files in the same top-level folder, TensorBoard will show them all together and let us flip between them. \n\nNext, let's go down where we were displaying the accuracy of our model. We need to add in calls to run our new summary operation here but instead of adding two new lines of code, we can just update these two lines. We can ask TensorFlow to run more than one operation in the same session.run statement. So we'll change this parameter from cost to an array of cost, summary. Now, session.run will return two results so we can capture the new result in the variable called training_summary. This is telling TensorFlow to run both operations at the same time and we're capturing both results in the two different variables. Since both operations need the same input data anyway, this is more efficient. \n\nNow, let's do the same thing for testing_summary. So we'll change cost to cost, summary. And then, we'll capture the result in a variable called testing_summary. Great, now we have the data stored in the training_summary and testing_summary variables.\n\nThe last step is to write that data into our log files. We'll call training_writer and we'll call .add_summary. We'll pass in the training_summary variable we just created and then we need to pass in the current epoch number. This will be the X axis in out graph. We'll pass in epoch. And now, let's do the same thing for the testing_writer. Testing_writer.add_summary. We'll pass in the testing_summary variable we just created. And then, again, the epoch. That's it, let's run the code."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create a summary operation to log the progress of the network\nwith tf.variable_scope('logging'):\n    tf.summary.scalar('current_cost', cost)\n    summary = tf.summary.merge_all()\n\n\n# Initialize a session so that we can run TensorFlow operations\nwith tf.Session() as session:\n\n    # Run the global variable initializer to initialize all variables and layers of the neural network\n    session.run(tf.global_variables_initializer())\n\n    # Create log file writers to record training progress.\n    # We'll store training and testing log data separately.\n    training_writer = tf.summary.FileWriter(\"./logs/training\", session.graph)\n    testing_writer = tf.summary.FileWriter(\"./logs/testing\", session.graph)\n\n    # Run the optimizer over and over to train the network.\n    # One epoch is one full run through the training data set.\n    for epoch in range(training_epochs):\n\n        # Feed in the training data and do one step of neural network training\n        session.run(optimizer, feed_dict={X: X_scaled_training, Y: Y_scaled_training})\n\n        # Every 5 training steps, log our progress\n        if epoch % 5 == 0:\n            # Get the current accuracy scores by running the \"cost\" operation on the training and test data sets\n            training_cost, training_summary = session.run([cost, summary], feed_dict={X: X_scaled_training, Y: Y_scaled_training})\n            testing_cost, testing_summary = session.run([cost, summary], feed_dict={X: X_scaled_testing, Y: Y_scaled_testing})\n\n            # Write the current training status to the log files (Which we can view with TensorBoard)\n            training_writer.add_summary(training_summary, epoch)\n            testing_writer.add_summary(testing_summary, epoch)\n\n\n            # Print the current training status to the screen\n            print(\"Epoch: {} - Training Cost: {}  Testing Cost: {}\".format(epoch, training_cost, testing_cost))\n\n    # Training is now complete!\n\n    # Get the final accuracy scores by running the \"cost\" operation on the training and test data sets\n    final_training_cost = session.run(cost, feed_dict={X: X_scaled_training, Y: Y_scaled_training})\n    final_testing_cost = session.run(cost, feed_dict={X: X_scaled_testing, Y: Y_scaled_testing})\n\n    print(\"Final Training cost: {}\".format(final_training_cost))\n    print(\"Final Testing cost: {}\".format(final_testing_cost))\n\n    # Now that the neural network is trained, let's use it to make predictions for our test data.\n    # Pass in the X testing data and run the \"prediciton\" operation\n    Y_predicted_scaled = session.run(prediction, feed_dict={X: X_scaled_testing})\n\n    # Unscale the data back to it's original units (dollars)\n    Y_predicted = Y_scaler.inverse_transform(Y_predicted_scaled)\n\n    real_earnings = test_data_df['total_earnings'].values[0]\n    predicted_earnings = Y_predicted[0][0]\n\n    print(\"The actual earnings of Game #1 were ${}\".format(real_earnings))\n    print(\"Our neural network predicted earnings of ${}\".format(predicted_earnings))",
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Epoch: 0 - Training Cost: 0.028926409780979156  Testing Cost: 0.0319664441049099\nEpoch: 5 - Training Cost: 0.013253812678158283  Testing Cost: 0.01492443960160017\nEpoch: 10 - Training Cost: 0.0061787692829966545  Testing Cost: 0.0071613066829741\nEpoch: 15 - Training Cost: 0.003138983156532049  Testing Cost: 0.003765603294596076\nEpoch: 20 - Training Cost: 0.0022207133006304502  Testing Cost: 0.0026913988403975964\nEpoch: 25 - Training Cost: 0.0018064930336549878  Testing Cost: 0.002134062582626939\nEpoch: 30 - Training Cost: 0.001251714420504868  Testing Cost: 0.0015351319452747703\nEpoch: 35 - Training Cost: 0.0010264781303703785  Testing Cost: 0.0013287374749779701\nEpoch: 40 - Training Cost: 0.0007947995327413082  Testing Cost: 0.0010392377153038979\nEpoch: 45 - Training Cost: 0.0006546201766468585  Testing Cost: 0.0008851231541484594\nEpoch: 50 - Training Cost: 0.0005391414160840213  Testing Cost: 0.000680678931530565\nEpoch: 55 - Training Cost: 0.0004510733706410974  Testing Cost: 0.0005768931587226689\nEpoch: 60 - Training Cost: 0.0003647185512818396  Testing Cost: 0.00046140048652887344\nEpoch: 65 - Training Cost: 0.00030327439890243113  Testing Cost: 0.00039974204264581203\nEpoch: 70 - Training Cost: 0.0002575911348685622  Testing Cost: 0.0003491428215056658\nEpoch: 75 - Training Cost: 0.00021733551693614572  Testing Cost: 0.0003000020224135369\nEpoch: 80 - Training Cost: 0.0001850560656748712  Testing Cost: 0.00026858053752221167\nEpoch: 85 - Training Cost: 0.00016155230696313083  Testing Cost: 0.00024403698625974357\nEpoch: 90 - Training Cost: 0.0001412541896570474  Testing Cost: 0.00022264417202677578\nEpoch: 95 - Training Cost: 0.00012484659964684397  Testing Cost: 0.00020502015831880271\nFinal Training cost: 0.00011402494419598952\nFinal Testing cost: 0.0001946239499375224\nThe actual earnings of Game #1 were $247537.0\nOur neural network predicted earnings of $252458.84375\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We generated the log files that we can view in TensorBoard, but hoe?\nTo run TensorBoard, we need to follow these steps:\n    1. Open up a terminal window and navigate to the folder where we've saved the logs files.\n    2. Write \"tensorboard --logdir=/logs\"\n    3. Find the text similiar to \"TensorBoard 1.13.1 at http://Maaz_Amjad:6006 (Press CTRL+C to quit)\"\n    4. Copy the text like \"http://Maaz_Amjad:6006\".\n    5. Open a new window, and paste this text there \"http://Maaz_Amjad:6006\"\n\nYour URL will be slightly different. Copy and paste the URL and then open up in your web browser. You might notice that the port number in the URL is always 6006. That's because 6006 looks kind of like G-O-O-G or goog for Google.\n\nOnce TensorBoard opens, click on the logging group. Now, you'll see an interactive chart for the metric. When you're done with TensorBoard, you can close it by going back to your terminal window and hitting Ctrl+C. So let's flip back to PyCharm and hit Ctrl+C."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Save the model file"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "So far, we've built and trained the model. Now, lets save that model to a file, so that we can reuse it later. We just have defined code to train the neural network. But, we want to save it, therefore, to save it, we first need to create a tf.train.Saver object. \n\n\nLet's put \"saver = tf.train.Saver()\" under the graph definition, but before the training loop. \"saver = tf.train.Saver()\", his is the object we'll use to save the model.\n\n###########################################################################\n\nsaver = tf.train.Saver()\n\n##########################################################################\n\nNow, at the very bottom of the code, We can save the model. To save the model, we just call \"saver.save\" and pass in the session and the file name. So, we'll call saver.save. We'll pass in the session and then the file name where we want to save it. We'll use log/trained_model.ckpt.\n\n\n#######################################################################################\n\nsave_path = saver.save(session, \"logs/trained_model.ckpt\")\nprint(\"Model saved: {}\".format(save_path))\n    \n########################################################################################\n\n\nThese files are called checkpoint files, so they're usually named with a .ckpt file name extension, but that's not required. Then, here on the next line, we'll just print out where the file was saved. Now, let's run the code."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create a summary operation to log the progress of the network\nwith tf.variable_scope('logging'):\n    tf.summary.scalar('current_cost', cost)\n    summary = tf.summary.merge_all()\n    \n###########################################################################\nsaver = tf.train.Saver()\n##########################################################################\n\n# Initialize a session so that we can run TensorFlow operations\nwith tf.Session() as session:\n\n    # Run the global variable initializer to initialize all variables and layers of the neural network\n    session.run(tf.global_variables_initializer())\n\n    # Create log file writers to record training progress.\n    # We'll store training and testing log data separately.\n    training_writer = tf.summary.FileWriter('./logs/training', session.graph)\n    testing_writer = tf.summary.FileWriter('./logs/testing', session.graph)\n\n    # Run the optimizer over and over to train the network.\n    # One epoch is one full run through the training data set.\n    for epoch in range(training_epochs):\n\n        # Feed in the training data and do one step of neural network training\n        session.run(optimizer, feed_dict={X: X_scaled_training, Y: Y_scaled_training})\n\n        # Every 5 training steps, log our progress\n        if epoch % 5 == 0:\n            # Get the current accuracy scores by running the \"cost\" operation on the training and test data sets\n            training_cost, training_summary = session.run([cost, summary], feed_dict={X: X_scaled_training, Y:Y_scaled_training})\n            testing_cost, testing_summary = session.run([cost, summary], feed_dict={X: X_scaled_testing, Y:Y_scaled_testing})\n\n            # Write the current training status to the log files (Which we can view with TensorBoard)\n            training_writer.add_summary(training_summary, epoch)\n            testing_writer.add_summary(testing_summary, epoch)\n\n            # Print the current training status to the screen\n            print(\"Epoch: {} - Training Cost: {}  Testing Cost: {}\".format(epoch, training_cost, testing_cost))\n\n    # Training is now complete!\n\n    # Get the final accuracy scores by running the \"cost\" operation on the training and test data sets\n    final_training_cost = session.run(cost, feed_dict={X: X_scaled_training, Y: Y_scaled_training})\n    final_testing_cost = session.run(cost, feed_dict={X: X_scaled_testing, Y: Y_scaled_testing})\n\n    print(\"Final Training cost: {}\".format(final_training_cost))\n    print(\"Final Testing cost: {}\".format(final_testing_cost))\n\n    # Now that the neural network is trained, let's use it to make predictions for our test data.\n    # Pass in the X testing data and run the \"prediciton\" operation\n    Y_predicted_scaled = session.run(prediction, feed_dict={X: X_scaled_testing})\n\n    # Unscale the data back to it's original units (dollars)\n    Y_predicted = Y_scaler.inverse_transform(Y_predicted_scaled)\n\n    real_earnings = test_data_df['total_earnings'].values[0]\n    predicted_earnings = Y_predicted[0][0]\n\n    print(\"The actual earnings of Game #1 were ${}\".format(real_earnings))\n    print(\"Our neural network predicted earnings of ${}\".format(predicted_earnings))\n\n#######################################################################################\n    save_path = saver.save(session, \"logs/trained_model.ckpt\")\n    print(\"Model saved: {}\".format(save_path))\n########################################################################################",
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Epoch: 0 - Training Cost: 0.0699923038482666  Testing Cost: 0.075189970433712\nEpoch: 5 - Training Cost: 0.024983476847410202  Testing Cost: 0.027623720467090607\nEpoch: 10 - Training Cost: 0.022632868960499763  Testing Cost: 0.02448413148522377\nEpoch: 15 - Training Cost: 0.01206369511783123  Testing Cost: 0.01419812347739935\nEpoch: 20 - Training Cost: 0.010984287597239017  Testing Cost: 0.012895396910607815\nEpoch: 25 - Training Cost: 0.006791762076318264  Testing Cost: 0.008036644198000431\nEpoch: 30 - Training Cost: 0.005904204212129116  Testing Cost: 0.00675926310941577\nEpoch: 35 - Training Cost: 0.004258265718817711  Testing Cost: 0.0048765563406050205\nEpoch: 40 - Training Cost: 0.003786633023992181  Testing Cost: 0.004197265487164259\nEpoch: 45 - Training Cost: 0.00292732915841043  Testing Cost: 0.0032303452026098967\nEpoch: 50 - Training Cost: 0.0021942986641079187  Testing Cost: 0.0024086751509457827\nEpoch: 55 - Training Cost: 0.0017885599518194795  Testing Cost: 0.001952003687620163\nEpoch: 60 - Training Cost: 0.001478765974752605  Testing Cost: 0.0016291679348796606\nEpoch: 65 - Training Cost: 0.0012246468104422092  Testing Cost: 0.0013632788322865963\nEpoch: 70 - Training Cost: 0.0009897258132696152  Testing Cost: 0.0010937743354588747\nEpoch: 75 - Training Cost: 0.0007941391668282449  Testing Cost: 0.0008877275395207107\nEpoch: 80 - Training Cost: 0.0006347922026179731  Testing Cost: 0.000715109403245151\nEpoch: 85 - Training Cost: 0.0005100453272461891  Testing Cost: 0.0005837354110553861\nEpoch: 90 - Training Cost: 0.00042563342140056193  Testing Cost: 0.00048858328955248\nEpoch: 95 - Training Cost: 0.0003598239563871175  Testing Cost: 0.00041502065141685307\nFinal Training cost: 0.00031776298419572413\nFinal Testing cost: 0.00037405051989480853\nThe actual earnings of Game #1 were $247537.0\nOur neural network predicted earnings of $234207.9375\nModel saved: logs/trained_model.ckpt\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Great, the model was saved. If we click here on the logs folder, we can also see that several new files were created. Now, let's see how to use the saved model. \n\nHere we have the same graph definition as before, wecan see that there's no training loop. Instead, let's go down where we are loading our checkpoint file. Just call saver.restore and pass in the session and the name of the file to load. So, we'll say session and the same file name, logs/trained_model.ckpt. \n\n############################################################################################\n    # Instead, load them from disk:\n    saver.restore(session, \"logs/trained_model.ckpt\")\n###########################################################################################\n\n\n**###Look Down** \"###Look at here!!!\" \nWe've commented out this line for a good reason. It's important that when loading from a checkpoint, we don't call tf.global_variables_initializer. Since all the variable values are loaded from a file, we don't want to initialize them back to their default values. Great, now we can use the pretrain network, just like before. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create a summary operation to log the progress of the network\nwith tf.variable_scope('logging'):\n    tf.summary.scalar('current_cost', cost)\n    summary = tf.summary.merge_all()\n\nsaver = tf.train.Saver()\n\n# Initialize a session so that we can run TensorFlow operations\nwith tf.Session() as session:\n\n######Look at here!!!     \n    \n    # When loading from a checkpoint, don't initialize the variables!\n    # session.run(tf.global_variables_initializer())\n    \n    \n\n############################################################################################\n    # Instead, load them from disk:\n    saver.restore(session, \"logs/trained_model.ckpt\")\n###########################################################################################\n    print(\"Trained model loaded from disk.\")\n\n    # Get the final accuracy scores by running the \"cost\" operation on the training and test data sets\n    training_cost = session.run(cost, feed_dict={X: X_scaled_training, Y: Y_scaled_training})\n    testing_cost = session.run(cost, feed_dict={X: X_scaled_testing, Y: Y_scaled_testing})\n\n    print(\"Final Training cost: {}\".format(training_cost))\n    print(\"Final Testing cost: {}\".format(testing_cost))\n\n    # Now that the neural network is trained, let's use it to make predictions for our test data.\n    # Pass in the X testing data and run the \"prediciton\" operation\n    Y_predicted_scaled = session.run(prediction, feed_dict={X: X_scaled_testing})\n\n    # Unscale the data back to it's original units (dollars)\n    Y_predicted = Y_scaler.inverse_transform(Y_predicted_scaled)\n\n    real_earnings = test_data_df['total_earnings'].values[0]\n    predicted_earnings = Y_predicted[0][0]\n\n    print(\"The actual earnings of Game #1 were ${}\".format(real_earnings))\n    print(\"Our neural network predicted earnings of ${}\".format(predicted_earnings))",
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": "WARNING:tensorflow:From /home/nbuser/anaconda3_501/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse standard file APIs to check for files with this prefix.\nINFO:tensorflow:Restoring parameters from logs/trained_model.ckpt\nTrained model loaded from disk.\nFinal Training cost: 0.00031776298419572413\nFinal Testing cost: 0.00037405051989480853\nThe actual earnings of Game #1 were $247537.0\nOur neural network predicted earnings of $234207.9375\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "we can see that we get the same result, using the pretrain model loaded from disk."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}