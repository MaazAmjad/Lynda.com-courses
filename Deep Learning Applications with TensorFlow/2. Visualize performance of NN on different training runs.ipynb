{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "## Visualize training runs\n\nWhen you are building machine learning models with TensorFlow, we'll be spending a lot of time training the model and then repeating the training process with different parameters to see what works best. Using TensorBoard we can visually monitor the progress of training as it happens, and even compare different training runs against each other.\n\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "we've defined the training filewriter and a testing filewriter that will write out log files during training that we can then view in TensorBoard. Let's say that we want to retrain this neural network several times, with a different number of nodes in the first layer each time.\n\n**Our goal** is to find out which neural network design gives us the best prediction accuracy. The problem is that each time we run the training process, additional log files will be created with the same name as the old log files. The new log files will mix with the old log files and create overlapping graphs in TensorBoard. We won't have any way to tell which training run was which."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": " We can fix this by using a different run name for each training run.\n \nLet's create the variable called RUN_NAME. This will just be a string where we can give a name to the current training run. Let's start with \"run 1 with 50 nodes\". \n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Currently every log file is named exactly the same way. Let's change how we are creating the log file names, so that we can separate them per run. All we have to do is put each run in a different subfolder. To do that, let's add in run name to the path where we are saving the log files. So here I'll add slash and then the placeholder, and then I'll substitute that in by calling .format and passing in the run name. And we'll do the same thing for the testing writer. .format, and pass in the run name. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior() \n\ntf.reset_default_graph() \n#In the case you want to re-run codeblock 3 (for what ever reason) just insert a simple tf.reset_default_graph()\n#at the beginning of the block. \n#This will reset the graph you have already create and though you can create it again.\n\n\n# Turn off TensorFlow warning messages in program output\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\n# Load training data set from CSV file\ntraining_data_df = pd.read_csv(\"sales_data_training.csv\", dtype=float)\n\n# Pull out columns for X (data to train with) and Y (value to predict)\nX_training = training_data_df.drop('total_earnings', axis=1).values\nY_training = training_data_df[['total_earnings']].values\n\n# Load testing data set from CSV file\ntest_data_df = pd.read_csv(\"sales_data_test.csv\", dtype=float)\n\n# Pull out columns for X (data to train with) and Y (value to predict)\nX_testing = test_data_df.drop('total_earnings', axis=1).values\nY_testing = test_data_df[['total_earnings']].values\n\n# All data needs to be scaled to a small range like 0 to 1 for the neural\n# network to work well. Create scalers for the inputs and outputs.\nX_scaler = MinMaxScaler(feature_range=(0, 1))\nY_scaler = MinMaxScaler(feature_range=(0, 1))\n\n# Scale both the training inputs and outputs\nX_scaled_training = X_scaler.fit_transform(X_training)\nY_scaled_training = Y_scaler.fit_transform(Y_training)\n\n# It's very important that the training and test data are scaled with the same scaler.\nX_scaled_testing = X_scaler.transform(X_testing)\nY_scaled_testing = Y_scaler.transform(Y_testing)\n\n# Define model parameters\n##########################################################\n# Adding RUN_NAME\n#########################################################\nRUN_NAME = \"run 1 with 50 nodes\"\nlearning_rate = 0.001\ntraining_epochs = 100\n\n# Define how many inputs and outputs are in our neural network\nnumber_of_inputs = 9\nnumber_of_outputs = 1\n\n# Define how many neurons we want in each layer of our neural network\nlayer_1_nodes = 50\nlayer_2_nodes = 100\nlayer_3_nodes = 50\n\n# Section One: Define the layers of the neural network itself\n\n# Input Layer\nwith tf.variable_scope('input'):\n    X = tf.placeholder(tf.float32, shape=(None, number_of_inputs), name=\"X\")\n\n# Layer 1\nwith tf.variable_scope('layer_1'):\n    weights = tf.get_variable(name=\"weights1\", shape=[number_of_inputs, layer_1_nodes], initializer=tf.glorot_uniform_initializer())\n    biases = tf.get_variable(name=\"biases1\", shape=[layer_1_nodes], initializer=tf.zeros_initializer())\n    layer_1_output = tf.nn.relu(tf.matmul(X, weights) + biases)\n\n# Layer 2\nwith tf.variable_scope('layer_2'):\n    weights = tf.get_variable(name=\"weights2\", shape=[layer_1_nodes, layer_2_nodes], initializer=tf.glorot_uniform_initializer())\n    biases = tf.get_variable(name=\"biases2\", shape=[layer_2_nodes], initializer=tf.zeros_initializer())\n    layer_2_output = tf.nn.relu(tf.matmul(layer_1_output, weights) + biases)\n\n# Layer 3\nwith tf.variable_scope('layer_3'):\n    weights = tf.get_variable(name=\"weights3\", shape=[layer_2_nodes, layer_3_nodes], initializer=tf.glorot_uniform_initializer())\n    biases = tf.get_variable(name=\"biases3\", shape=[layer_3_nodes], initializer=tf.zeros_initializer())\n    layer_3_output = tf.nn.relu(tf.matmul(layer_2_output, weights) + biases)\n\n# Output Layer\nwith tf.variable_scope('output'):\n    weights = tf.get_variable(name=\"weights4\", shape=[layer_3_nodes, number_of_outputs], initializer=tf.glorot_uniform_initializer())\n    biases = tf.get_variable(name=\"biases4\", shape=[number_of_outputs], initializer=tf.zeros_initializer())\n    prediction = tf.matmul(layer_3_output, weights) + biases\n\n# Section Two: Define the cost function of the neural network that will be optimized during training\n\nwith tf.variable_scope('cost'):\n    Y = tf.placeholder(tf.float32, shape=(None, 1), name=\"Y\")\n    cost = tf.reduce_mean(tf.squared_difference(prediction, Y))\n\n# Section Three: Define the optimizer function that will be run to optimize the neural network\n\nwith tf.variable_scope('train'):\n    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n\n# Create a summary operation to log the progress of the network\nwith tf.variable_scope('logging'):\n    tf.summary.scalar('current_cost', cost)\n    summary = tf.summary.merge_all()\n\n# Initialize a session so that we can run TensorFlow operations\nwith tf.Session() as session:\n\n    # Run the global variable initializer to initialize all variables and layers of the neural network\n    session.run(tf.global_variables_initializer())\n\n    \n\n    # Create log file writers to record training progress.\n    # We'll store training and testing log data separately.\n    \n###############################################################\n# Saving Different log file names\n##############################################################\n    training_writer = tf.summary.FileWriter(\"./logs/{}/training\".format(RUN_NAME), session.graph)\n    testing_writer = tf.summary.FileWriter(\"./logs/{}/testing\".format(RUN_NAME), session.graph)\n\n    # Run the optimizer over and over to train the network.\n    # One epoch is one full run through the training data set.\n    for epoch in range(training_epochs):\n\n        # Feed in the training data and do one step of neural network training\n        session.run(optimizer, feed_dict={X: X_scaled_training, Y: Y_scaled_training})\n\n        # Every few training steps, log our progress\n        if epoch % 5 == 0:\n            # Get the current accuracy scores by running the \"cost\" operation on the training and test data sets\n            training_cost, training_summary = session.run([cost, summary], feed_dict={X: X_scaled_training, Y:Y_scaled_training})\n            testing_cost, testing_summary = session.run([cost, summary], feed_dict={X: X_scaled_testing, Y:Y_scaled_testing})\n\n            # Write the current training status to the log files (Which we can view with TensorBoard)\n            training_writer.add_summary(training_summary, epoch)\n            testing_writer.add_summary(testing_summary, epoch)\n\n            # Print the current training status to the screen\n            print(\"Epoch: {} - Training Cost: {}  Testing Cost: {}\".format(epoch, training_cost, testing_cost))\n\n    # Training is now complete!\n\n    # Get the final accuracy scores by running the \"cost\" operation on the training and test data sets\n    final_training_cost = session.run(cost, feed_dict={X: X_scaled_training, Y: Y_scaled_training})\n    final_testing_cost = session.run(cost, feed_dict={X: X_scaled_testing, Y: Y_scaled_testing})\n\n    print(\"Final Training cost: {}\".format(final_training_cost))\n    print(\"Final Testing cost: {}\".format(final_testing_cost))\n",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Epoch: 0 - Training Cost: 0.0998048335313797  Testing Cost: 0.10595479607582092\nEpoch: 5 - Training Cost: 0.031163925305008888  Testing Cost: 0.032601162791252136\nEpoch: 10 - Training Cost: 0.022592948749661446  Testing Cost: 0.023315947502851486\nEpoch: 15 - Training Cost: 0.010349605232477188  Testing Cost: 0.011542453430593014\nEpoch: 20 - Training Cost: 0.009305378422141075  Testing Cost: 0.010597547516226768\nEpoch: 25 - Training Cost: 0.0045861657708883286  Testing Cost: 0.005110942758619785\nEpoch: 30 - Training Cost: 0.004168755374848843  Testing Cost: 0.004293614998459816\nEpoch: 35 - Training Cost: 0.002657415345311165  Testing Cost: 0.0028235369827598333\nEpoch: 40 - Training Cost: 0.00260140816681087  Testing Cost: 0.0028646260034292936\nEpoch: 45 - Training Cost: 0.001934573519974947  Testing Cost: 0.002011549659073353\nEpoch: 50 - Training Cost: 0.001592321670614183  Testing Cost: 0.0016639715759083629\nEpoch: 55 - Training Cost: 0.0013818895677104592  Testing Cost: 0.0014728999231010675\nEpoch: 60 - Training Cost: 0.0011095189256593585  Testing Cost: 0.0011709961108863354\nEpoch: 65 - Training Cost: 0.0009455688996240497  Testing Cost: 0.0010079039493575692\nEpoch: 70 - Training Cost: 0.0008290207479149103  Testing Cost: 0.00090861163334921\nEpoch: 75 - Training Cost: 0.0007074220338836312  Testing Cost: 0.0007927495753392577\nEpoch: 80 - Training Cost: 0.0006159927579574287  Testing Cost: 0.0007129697478376329\nEpoch: 85 - Training Cost: 0.000538766267709434  Testing Cost: 0.0006384943262673914\nEpoch: 90 - Training Cost: 0.0004725283070001751  Testing Cost: 0.0005733479629270732\nEpoch: 95 - Training Cost: 0.00041813470306806266  Testing Cost: 0.0005231762188486755\nFinal Training cost: 0.0003812462091445923\nFinal Testing cost: 0.00048817138304002583\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's try tweaking our neural network and doing a second run.\n\nFirst, let's redefined our  RUN_Namre=\"run 2 with 20 nodes\".\nAnd then let's edit the number of nodes in the first layer to be 20."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior() \n\ntf.reset_default_graph() \n#In the case you want to re-run codeblock 3 (for what ever reason) just insert a simple tf.reset_default_graph()\n#at the beginning of the block. \n#This will reset the graph you have already create and though you can create it again.\n\n\n# Turn off TensorFlow warning messages in program output\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\n# Load training data set from CSV file\ntraining_data_df = pd.read_csv(\"sales_data_training.csv\", dtype=float)\n\n# Pull out columns for X (data to train with) and Y (value to predict)\nX_training = training_data_df.drop('total_earnings', axis=1).values\nY_training = training_data_df[['total_earnings']].values\n\n# Load testing data set from CSV file\ntest_data_df = pd.read_csv(\"sales_data_test.csv\", dtype=float)\n\n# Pull out columns for X (data to train with) and Y (value to predict)\nX_testing = test_data_df.drop('total_earnings', axis=1).values\nY_testing = test_data_df[['total_earnings']].values\n\n# All data needs to be scaled to a small range like 0 to 1 for the neural\n# network to work well. Create scalers for the inputs and outputs.\nX_scaler = MinMaxScaler(feature_range=(0, 1))\nY_scaler = MinMaxScaler(feature_range=(0, 1))\n\n# Scale both the training inputs and outputs\nX_scaled_training = X_scaler.fit_transform(X_training)\nY_scaled_training = Y_scaler.fit_transform(Y_training)\n\n# It's very important that the training and test data are scaled with the same scaler.\nX_scaled_testing = X_scaler.transform(X_testing)\nY_scaled_testing = Y_scaler.transform(Y_testing)\n\n# Define model parameters\n##########################################################\n# Adding RUN_NAME\n#########################################################\nRUN_NAME = \"run 2 with 20 nodes\"\nlearning_rate = 0.001\ntraining_epochs = 100\n\n# Define how many inputs and outputs are in our neural network\nnumber_of_inputs = 9\nnumber_of_outputs = 1\n\n# Define how many neurons we want in each layer of our neural network\nlayer_1_nodes = 20\nlayer_2_nodes = 100\nlayer_3_nodes = 50\n\n# Section One: Define the layers of the neural network itself\n\n# Input Layer\nwith tf.variable_scope('input'):\n    X = tf.placeholder(tf.float32, shape=(None, number_of_inputs), name=\"X\")\n\n# Layer 1\nwith tf.variable_scope('layer_1'):\n    weights = tf.get_variable(name=\"weights1\", shape=[number_of_inputs, layer_1_nodes], initializer=tf.glorot_uniform_initializer())\n    biases = tf.get_variable(name=\"biases1\", shape=[layer_1_nodes], initializer=tf.zeros_initializer())\n    layer_1_output = tf.nn.relu(tf.matmul(X, weights) + biases)\n\n# Layer 2\nwith tf.variable_scope('layer_2'):\n    weights = tf.get_variable(name=\"weights2\", shape=[layer_1_nodes, layer_2_nodes], initializer=tf.glorot_uniform_initializer())\n    biases = tf.get_variable(name=\"biases2\", shape=[layer_2_nodes], initializer=tf.zeros_initializer())\n    layer_2_output = tf.nn.relu(tf.matmul(layer_1_output, weights) + biases)\n\n# Layer 3\nwith tf.variable_scope('layer_3'):\n    weights = tf.get_variable(name=\"weights3\", shape=[layer_2_nodes, layer_3_nodes], initializer=tf.glorot_uniform_initializer())\n    biases = tf.get_variable(name=\"biases3\", shape=[layer_3_nodes], initializer=tf.zeros_initializer())\n    layer_3_output = tf.nn.relu(tf.matmul(layer_2_output, weights) + biases)\n\n# Output Layer\nwith tf.variable_scope('output'):\n    weights = tf.get_variable(name=\"weights4\", shape=[layer_3_nodes, number_of_outputs], initializer=tf.glorot_uniform_initializer())\n    biases = tf.get_variable(name=\"biases4\", shape=[number_of_outputs], initializer=tf.zeros_initializer())\n    prediction = tf.matmul(layer_3_output, weights) + biases\n\n# Section Two: Define the cost function of the neural network that will be optimized during training\n\nwith tf.variable_scope('cost'):\n    Y = tf.placeholder(tf.float32, shape=(None, 1), name=\"Y\")\n    cost = tf.reduce_mean(tf.squared_difference(prediction, Y))\n\n# Section Three: Define the optimizer function that will be run to optimize the neural network\n\nwith tf.variable_scope('train'):\n    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n\n# Create a summary operation to log the progress of the network\nwith tf.variable_scope('logging'):\n    tf.summary.scalar('current_cost', cost)\n    summary = tf.summary.merge_all()\n\n# Initialize a session so that we can run TensorFlow operations\nwith tf.Session() as session:\n\n    # Run the global variable initializer to initialize all variables and layers of the neural network\n    session.run(tf.global_variables_initializer())\n\n    \n\n    # Create log file writers to record training progress.\n    # We'll store training and testing log data separately.\n    \n###############################################################\n# Saving Different log file names\n##############################################################\n    training_writer = tf.summary.FileWriter(\"./logs/{}/training\".format(RUN_NAME), session.graph)\n    testing_writer = tf.summary.FileWriter(\"./logs/{}/testing\".format(RUN_NAME), session.graph)\n\n    # Run the optimizer over and over to train the network.\n    # One epoch is one full run through the training data set.\n    for epoch in range(training_epochs):\n\n        # Feed in the training data and do one step of neural network training\n        session.run(optimizer, feed_dict={X: X_scaled_training, Y: Y_scaled_training})\n\n        # Every few training steps, log our progress\n        if epoch % 5 == 0:\n            # Get the current accuracy scores by running the \"cost\" operation on the training and test data sets\n            training_cost, training_summary = session.run([cost, summary], feed_dict={X: X_scaled_training, Y:Y_scaled_training})\n            testing_cost, testing_summary = session.run([cost, summary], feed_dict={X: X_scaled_testing, Y:Y_scaled_testing})\n\n            # Write the current training status to the log files (Which we can view with TensorBoard)\n            training_writer.add_summary(training_summary, epoch)\n            testing_writer.add_summary(testing_summary, epoch)\n\n            # Print the current training status to the screen\n            print(\"Epoch: {} - Training Cost: {}  Testing Cost: {}\".format(epoch, training_cost, testing_cost))\n\n    # Training is now complete!\n\n    # Get the final accuracy scores by running the \"cost\" operation on the training and test data sets\n    final_training_cost = session.run(cost, feed_dict={X: X_scaled_training, Y: Y_scaled_training})\n    final_testing_cost = session.run(cost, feed_dict={X: X_scaled_testing, Y: Y_scaled_testing})\n\n    print(\"Final Training cost: {}\".format(final_training_cost))\n    print(\"Final Testing cost: {}\".format(final_testing_cost))\n",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Epoch: 0 - Training Cost: 0.11850441247224808  Testing Cost: 0.12488681823015213\nEpoch: 5 - Training Cost: 0.022354397922754288  Testing Cost: 0.024478714913129807\nEpoch: 10 - Training Cost: 0.033058442175388336  Testing Cost: 0.033243052661418915\nEpoch: 15 - Training Cost: 0.019600527361035347  Testing Cost: 0.020515475422143936\nEpoch: 20 - Training Cost: 0.015040948987007141  Testing Cost: 0.016889413818717003\nEpoch: 25 - Training Cost: 0.015711022540926933  Testing Cost: 0.017822954803705215\nEpoch: 30 - Training Cost: 0.012900685891509056  Testing Cost: 0.014597556553781033\nEpoch: 35 - Training Cost: 0.011000080034136772  Testing Cost: 0.012180657126009464\nEpoch: 40 - Training Cost: 0.010341642424464226  Testing Cost: 0.01130259782075882\nEpoch: 45 - Training Cost: 0.008823253214359283  Testing Cost: 0.009784952737390995\nEpoch: 50 - Training Cost: 0.007908898405730724  Testing Cost: 0.008856650441884995\nEpoch: 55 - Training Cost: 0.006876008119434118  Testing Cost: 0.00772795919328928\nEpoch: 60 - Training Cost: 0.005794705357402563  Testing Cost: 0.006547736003994942\nEpoch: 65 - Training Cost: 0.004851925652474165  Testing Cost: 0.0054667615331709385\nEpoch: 70 - Training Cost: 0.004089219029992819  Testing Cost: 0.004599535372108221\nEpoch: 75 - Training Cost: 0.0034248619340360165  Testing Cost: 0.003856525756418705\nEpoch: 80 - Training Cost: 0.002918075304478407  Testing Cost: 0.003249750705435872\nEpoch: 85 - Training Cost: 0.0025178927462548018  Testing Cost: 0.002747687976807356\nEpoch: 90 - Training Cost: 0.0022034707944840193  Testing Cost: 0.002353465184569359\nEpoch: 95 - Training Cost: 0.0019265852170065045  Testing Cost: 0.00203480152413249\nFinal Training cost: 0.0017284407513216138\nFinal Testing cost: 0.0018322499236091971\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "And now we have a second folder. Now let's open up TensorBoard and see how these runs look. Let's open up a terminal window. \n\nOpen terminal and type \"tensorboard --logdir=05/logs\".\n\nDirectory should be where our logs files are stored.\n\nWhen TensorBoard starts it will give you a URL to open in your browser. Copy and paste that into your web browser. Okay, now click on logging to see our charts and let's click to expand the chart. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Custom data Visulization: Add custom visualizations to TensorBoard"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "TensorBoard allows us to create custom visualizations beyond just line graphs. We can use these visualizations to monitor your machine learning model and what kind of data it's generating. Currently TensorFlow supports these types of visualizations. First, the image visualization allows us to see any array of data as an image. We can create an image visualization by adding a tf.summary.image object to your graph and passing it the array you want to visualize. This is helpful when you're building a neural network that classifies or generates images.\n\nWe can also listen to audio data in TensorBoard. To add an audio player to TensorBoard, we can create a new tf.summary.audio object and you add it to your computational graph, this is typically used when building models that recognize speech or generate sounds. It lets us hear the sound files that our model is processing.\n\nWe can also create interactive histograms and distribution graphs in TensorBoard. When you add a tf.summary.histogram object to our computational graph, it creates both a histogram like you see here, and a distribution graph. Histograms are a powerful way to monitor ranges of values over time, they show us not only the range of values in the array of data but they can also display how these data ranges vary through time."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "![Custom Data Visulization](images/data_viz.png)"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "\n\nLet's try adding a histagram summary to a neural network and see how it works. \n\nwe've already defined our computational graph in the training loop. We also already have a scalar summary defined that monitors the cost of our neural network as it's trained. This will generate a line chart that shows the neural network getting more accurate over time during the training process. But let's say we want to visualize the actual predictions that our neural network is making during the training process. To do that, we can add a histogram that monitors our predictions.\n\nLet's create a new tf.summary.histogram node. Tf.summary.histogram then we'll pass in the name for our node in this case we'll use predicted value. And then we pass in the node in our graph that we want to monitor, which will be prediction. And that's all we have to do. Since we already called tf.summary.merge_all on the next line, any new summary metrics we add to our graph will automatically get picked up and added to our log files. Let's run the code to train our network and generate log files that we can view in TensorBoard. But first, if you already have a log subfolder, delete that before continuing. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior() \ntf.reset_default_graph() \n\n\n# Turn off TensorFlow warning messages in program output\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\n# Load training data set from CSV file\ntraining_data_df = pd.read_csv(\"sales_data_training.csv\", dtype=float)\n\n# Pull out columns for X (data to train with) and Y (value to predict)\nX_training = training_data_df.drop('total_earnings', axis=1).values\nY_training = training_data_df[['total_earnings']].values\n\n# Load testing data set from CSV file\ntest_data_df = pd.read_csv(\"sales_data_test.csv\", dtype=float)\n\n# Pull out columns for X (data to train with) and Y (value to predict)\nX_testing = test_data_df.drop('total_earnings', axis=1).values\nY_testing = test_data_df[['total_earnings']].values\n\n# All data needs to be scaled to a small range like 0 to 1 for the neural\n# network to work well. Create scalers for the inputs and outputs.\nX_scaler = MinMaxScaler(feature_range=(0, 1))\nY_scaler = MinMaxScaler(feature_range=(0, 1))\n\n# Scale both the training inputs and outputs\nX_scaled_training = X_scaler.fit_transform(X_training)\nY_scaled_training = Y_scaler.fit_transform(Y_training)\n\n# It's very important that the training and test data are scaled with the same scaler.\nX_scaled_testing = X_scaler.transform(X_testing)\nY_scaled_testing = Y_scaler.transform(Y_testing)\n\n# Define model parameters\nRUN_NAME = \"histogram_visualization\"\nlearning_rate = 0.001\ntraining_epochs = 100\n\n# Define how many inputs and outputs are in our neural network\nnumber_of_inputs = 9\nnumber_of_outputs = 1\n\n# Define how many neurons we want in each layer of our neural network\nlayer_1_nodes = 50\nlayer_2_nodes = 100\nlayer_3_nodes = 50\n\n# Section One: Define the layers of the neural network itself\n\n\n# Input Layer\nwith tf.variable_scope('input'):\n    X = tf.placeholder(tf.float32, shape=(None, number_of_inputs), name=\"X\")\n\n# Layer 1\nwith tf.variable_scope('layer_1'):\n    weights = tf.get_variable(name=\"weights1\", shape=[number_of_inputs, layer_1_nodes], initializer=tf.glorot_uniform_initializer())\n    biases = tf.get_variable(name=\"biases1\", shape=[layer_1_nodes], initializer=tf.zeros_initializer())\n    layer_1_output = tf.nn.relu(tf.matmul(X, weights) + biases)\n\n# Layer 2\nwith tf.variable_scope('layer_2'):\n    weights = tf.get_variable(name=\"weights2\", shape=[layer_1_nodes, layer_2_nodes], initializer=tf.glorot_uniform_initializer())\n    biases = tf.get_variable(name=\"biases2\", shape=[layer_2_nodes], initializer=tf.zeros_initializer())\n    layer_2_output = tf.nn.relu(tf.matmul(layer_1_output, weights) + biases)\n\n# Layer 3\nwith tf.variable_scope('layer_3'):\n    weights = tf.get_variable(name=\"weights3\", shape=[layer_2_nodes, layer_3_nodes], initializer=tf.glorot_uniform_initializer())\n    biases = tf.get_variable(name=\"biases3\", shape=[layer_3_nodes], initializer=tf.zeros_initializer())\n    layer_3_output = tf.nn.relu(tf.matmul(layer_2_output, weights) + biases)\n\n# Output Layer\nwith tf.variable_scope('output'):\n    weights = tf.get_variable(name=\"weights4\", shape=[layer_3_nodes, number_of_outputs], initializer=tf.glorot_uniform_initializer())\n    biases = tf.get_variable(name=\"biases4\", shape=[number_of_outputs], initializer=tf.zeros_initializer())\n    prediction = tf.matmul(layer_3_output, weights) + biases\n\n# Section Two: Define the cost function of the neural network that will be optimized during training\n\nwith tf.variable_scope('cost'):\n    Y = tf.placeholder(tf.float32, shape=(None, 1), name=\"Y\")\n    cost = tf.reduce_mean(tf.squared_difference(prediction, Y))\n\n# Section Three: Define the optimizer function that will be run to optimize the neural network\n\nwith tf.variable_scope('train'):\n    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n\n####################################################################################\n# Adding Histogram here\n####################################################################################\n    \n# Create a summary operation to log the progress of the network\nwith tf.variable_scope('logging'):\n    tf.summary.scalar('current_cost', cost)\n    tf.summary.histogram('predicted_value', prediction)\n    summary = tf.summary.merge_all()\n\n# Initialize a session so that we can run TensorFlow operations\nwith tf.Session() as session:\n\n    # Run the global variable initializer to initialize all variables and layers of the neural network\n    session.run(tf.global_variables_initializer())\n\n    # Create log file writers to record training progress.\n    # We'll store training and testing log data separately.\n    training_writer = tf.summary.FileWriter(\"./logs/{}/training\".format(RUN_NAME), session.graph)\n    testing_writer = tf.summary.FileWriter(\"./logs/{}/testing\".format(RUN_NAME), session.graph)\n\n    # Run the optimizer over and over to train the network.\n    # One epoch is one full run through the training data set.\n    for epoch in range(training_epochs):\n\n        # Feed in the training data and do one step of neural network training\n        session.run(optimizer, feed_dict={X: X_scaled_training, Y: Y_scaled_training})\n\n        # Every few training steps, log our progress\n        if epoch % 5 == 0:\n            # Get the current accuracy scores by running the \"cost\" operation on the training and test data sets\n            training_cost, training_summary = session.run([cost, summary], feed_dict={X: X_scaled_training, Y:Y_scaled_training})\n            testing_cost, testing_summary = session.run([cost, summary], feed_dict={X: X_scaled_testing, Y:Y_scaled_testing})\n\n            # Write the current training status to the log files (Which we can view with TensorBoard)\n            training_writer.add_summary(training_summary, epoch)\n            testing_writer.add_summary(testing_summary, epoch)\n\n            # Print the current training status to the screen\n            print(\"Epoch: {} - Training Cost: {}  Testing Cost: {}\".format(epoch, training_cost, testing_cost))\n\n    # Training is now complete!\n\n    # Get the final accuracy scores by running the \"cost\" operation on the training and test data sets\n    final_training_cost = session.run(cost, feed_dict={X: X_scaled_training, Y: Y_scaled_training})\n    final_testing_cost = session.run(cost, feed_dict={X: X_scaled_testing, Y: Y_scaled_testing})\n\n    print(\"Final Training cost: {}\".format(final_training_cost))\n    print(\"Final Testing cost: {}\".format(final_testing_cost))\n",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Epoch: 0 - Training Cost: 0.016277538612484932  Testing Cost: 0.01728086546063423\nEpoch: 5 - Training Cost: 0.008747064508497715  Testing Cost: 0.009324006736278534\nEpoch: 10 - Training Cost: 0.0066581652499735355  Testing Cost: 0.006988320965319872\nEpoch: 15 - Training Cost: 0.005339732393622398  Testing Cost: 0.005356414243578911\nEpoch: 20 - Training Cost: 0.003938702400773764  Testing Cost: 0.004059947095811367\nEpoch: 25 - Training Cost: 0.002515074796974659  Testing Cost: 0.002571344142779708\nEpoch: 30 - Training Cost: 0.0016279771225526929  Testing Cost: 0.0018159097526222467\nEpoch: 35 - Training Cost: 0.0011463374830782413  Testing Cost: 0.0013736814726144075\nEpoch: 40 - Training Cost: 0.0008409644942730665  Testing Cost: 0.0010576178319752216\nEpoch: 45 - Training Cost: 0.00064996094442904  Testing Cost: 0.0008440464735031128\nEpoch: 50 - Training Cost: 0.0005057525704614818  Testing Cost: 0.0007262466824613512\nEpoch: 55 - Training Cost: 0.0003924186748918146  Testing Cost: 0.0005972191574983299\nEpoch: 60 - Training Cost: 0.0003361139097250998  Testing Cost: 0.0005220091552473605\nEpoch: 65 - Training Cost: 0.0002900221443269402  Testing Cost: 0.00047172477934509516\nEpoch: 70 - Training Cost: 0.0002412744506727904  Testing Cost: 0.00039667694363743067\nEpoch: 75 - Training Cost: 0.00020745655638165772  Testing Cost: 0.0003464030451141298\nEpoch: 80 - Training Cost: 0.000182007672265172  Testing Cost: 0.0003179977065883577\nEpoch: 85 - Training Cost: 0.0001577383663970977  Testing Cost: 0.0002867439470719546\nEpoch: 90 - Training Cost: 0.0001396051811752841  Testing Cost: 0.00026016435003839433\nEpoch: 95 - Training Cost: 0.0001228930923389271  Testing Cost: 0.00023682967002969235\nFinal Training cost: 0.00011182264279341325\nFinal Testing cost: 0.0002187015925301239\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}